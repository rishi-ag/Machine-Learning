for(col in 1:ncol(order.mat)) {
class.mat[row, col] <- labels[order.mat[row, col]]
}
}
class.prob <- as.data.frame(t(apply(X = class.mat, MARGIN =  1,
FUN = function(row){LabelCount(row, class.type)/k})))
names(class.prob) <- class.type
return(class.prob)
}
#--------------------------------------------------------------------------------
#Main function for classifying Data
kNN_classifier <- function(features, memory = NULL, labels,
operation = "train", k = 1, p = 2) {
#Input Validation
not_empty(features)
not_empty(labels)
is.count(k)
is.count(p)
assert_that(k %% 2 == 1)
assert_that(p %in% c(1,2,Inf))
assert_that(operation %in% c("train", "predict"))
if(operation == "predict") not_empty(memory)
class.type <- unique(labels)
#Calculate Order Matrix
if(operation == "train") {
class.prob <- Train(features, labels, class.type, k, p)
} else {
class.prob <- Predict(features, memory, labels, class.type, k, p)
}
#Predict classes
predicted.class <- apply(class.prob, 1, function(row) class.type[which.max(row)])
#Calculate error
error <- accuracy <- NA
if(operation == "train"){
error <- table(predicted.class, labels)
accuracy <- mean(predicted.class == labels )
} else {
error <- table(predicted.class, features[,3])
accuracy <- mean(predicted.class == features[,3])
}
return(list(pred.labels = predicted.class,
error = error,
accuracy = accuracy,
class.prob = class.prob))
}
#--------------------------------------------------------------------------------------------------
#Demo
features <- genWaves()
labels <- features$y
summary <- kNN_classifier(features = features, memory = NULL, labels = labels,
operation = "train", k = 5, p = 2)
summary$pred.labels
summary$error
summary$accuracy
summary$class.prob
labels <- features$y
features <- genWaves()
features <- genWaves()
labels <- features$y
k_seq <- seq.int(from = 1, to = 31, by = 2)
kNNClassifier <- function(features, memory = NULL, labels,
operation = "train", k = 1, p = 2) {
#Input Validation
not_empty(features)
not_empty(labels)
is.count(k)
is.count(p)
assert_that(k %% 2 == 1)
assert_that(p %in% c(1,2,Inf))
assert_that(operation %in% c("train", "predict"))
if(operation == "predict") not_empty(memory)
class.type <- unique(labels)
#Calculate Order Matrix
if(operation == "train") {
class.prob <- Train(features, labels, class.type, k, p)
} else {
class.prob <- Predict(features, memory, labels, class.type, k, p)
}
#Predict classes
predicted.class <- apply(class.prob, 1, function(row) class.type[which.max(row)])
#Calculate error
error <- accuracy <- NA
if(operation == "train"){
error <- table(predicted.class, labels)
accuracy <- mean(predicted.class == labels )
} else {
error <- table(predicted.class, features[,3])
accuracy <- mean(predicted.class == features[,3])
}
return(list(pred.labels = predicted.class,
error = error,
accuracy = accuracy,
class.prob = class.prob))
}
KNNClassifier <- function(features, memory = NULL, labels,
operation = "train", k = 1, p = 2) {
#Input Validation
not_empty(features)
not_empty(labels)
is.count(k)
is.count(p)
assert_that(k %% 2 == 1)
assert_that(p %in% c(1,2,Inf))
assert_that(operation %in% c("train", "predict"))
if(operation == "predict") not_empty(memory)
class.type <- unique(labels)
#Calculate Order Matrix
if(operation == "train") {
class.prob <- Train(features, labels, class.type, k, p)
} else {
class.prob <- Predict(features, memory, labels, class.type, k, p)
}
#Predict classes
predicted.class <- apply(class.prob, 1, function(row) class.type[which.max(row)])
#Calculate error
error <- accuracy <- NA
if(operation == "train"){
error <- table(predicted.class, labels)
accuracy <- mean(predicted.class == labels )
} else {
error <- table(predicted.class, features[,3])
accuracy <- mean(predicted.class == features[,3])
}
return(list(pred.labels = predicted.class,
error = error,
accuracy = accuracy,
class.prob = class.prob))
}
#--------------------------------------------------------------------------------------------------
#Demo
k_seq <- seq.int(from = 1, to = 31, by = 2)
train_error <- sapply(k_seq,
function(k) {
(1 - KNNClassifier(features =  features, labels = labels, operation = "train", k = k)$accuracy)
})
data.frame(k = k_seq, train = train_error)
error.DF <- data.frame(k = k_seq, train = train_error)
View(error.DF)
View(error.DF)
error.DF <- data.frame(k = k_seq, error = train_error)
ggplot(data = error.DF) +
geom_point(aes(x = k, y = error))
ggplot(data = error.DF) +
geom_point(aes(x = k, y = error)) +
geom_line(aes(x = k, y = error))
error.DF
which.min(error.DF$error)
best.k <- error.DF$k[which.min(error.DF$error)]
best.k
summary <- KNNClassifier(features = features, memory = NULL, labels = labels,
operation = "train", k = best.k, p = 2)
summary$accuracy
features <- genWaves()
labels <- features$y
k_seq <- seq.int(from = 1, to = 31, by = 2)
train_error <- sapply(k_seq,
function(k) {
(1 - KNNClassifier(features =  features, labels = labels, operation = "train", k = k)$accuracy)
})
#data frams of errors with differet k
error.DF <- data.frame(k = k_seq, error = train_error)
#plot
ggplot(data = error.DF) +
geom_point(aes(x = k, y = error)) +
geom_line(aes(x = k, y = error))
best.k <- error.DF$k[which.min(error.DF$error)]
#train with best k
summary <- KNNClassifier(features = features, memory = NULL, labels = labels,
operation = "train", k = best.k, p = 2)
operation = "train"
p = 2
operation = "train"
p = 2
k = 5
features <- genWaves()
labels <- features$y
not_empty(features)
not_empty(labels)
is.count(k)
is.count(p)
assert_that(k %% 2 == 1)
assert_that(p %in% c(1,2,Inf))
assert_that(p %in% c(1,2,Inf))
assert_that(operation %in% c("train", "predict"))
if(operation == "predict") not_empty(memory)
class.type <- unique(labels)
#Calculate Order Matrix
if(operation == "train") {
class.prob <- Train(features, labels, class.type, k, p)
} else {
class.prob <- Predict(features, memory, labels, class.type, k, p)
}
predicted.class <- t(apply(class.prob, 1, function(row) c(type = class.type[which.max(row)], prob = max(row))))
predicted.class
head(predicted.class)
predicted.class$type
predicted.class <- data.frame(t(apply(class.prob, 1, function(row) c(type = class.type[which.max(row)], prob = max(row)))))
predicted.class$type
KNNClassifier <- function(features, memory = NULL, labels,
operation = "train", k = 1, p = 2) {
#Input Validation
not_empty(features)
not_empty(labels)
is.count(k)
is.count(p)
assert_that(k %% 2 == 1)
assert_that(p %in% c(1,2,Inf))
assert_that(operation %in% c("train", "predict"))
if(operation == "predict") not_empty(memory)
class.type <- unique(labels)
#Calculate Order Matrix
if(operation == "train") {
class.prob <- Train(features, labels, class.type, k, p)
} else {
class.prob <- Predict(features, memory, labels, class.type, k, p)
}
#Predict classes
predicted.class <- data.frame(t(apply(class.prob, 1, function(row) c(type = class.type[which.max(row)], prob = max(row)))))
error <- accuracy <- NA
if(operation == "train"){
error <- table(predicted.class$type, labels)
accuracy <- mean(predicted.class$type == labels )
} else {
error <- table(predicted.class$type, features[,3])
accuracy <- mean(predicted.class$prob == features[,3])
}
return(list(pred.labels = predicted.class$type,
error = error,
accuracy = accuracy,
class.prob = predicted.class$prob))
}
features <- genWaves()
labels <- features$y
k_seq <- seq.int(from = 1, to = 31, by = 2)
train_error <- sapply(k_seq,
function(k) {
(1 - KNNClassifier(features =  features, labels = labels, operation = "train", k = k)$accuracy)
})
#data frams of errors with differet k
error.DF <- data.frame(k = k_seq, error = train_error)
#plot
ggplot(data = error.DF) +
geom_point(aes(x = k, y = error)) +
geom_line(aes(x = k, y = error))
#choose best k
best.k <- error.DF$k[which.min(error.DF$error)]
#train with best k
summary <- KNNClassifier(features = features, memory = NULL, labels = labels,
operation = "train", k = best.k, p = 2)
summary$accuracy
KNNClassifier <- function(features, memory = NULL, labels,
operation = "train", k = 1, p = 2) {
#Input Validation
not_empty(features)
not_empty(labels)
is.count(k)
is.count(p)
assert_that(k %% 2 == 1)
assert_that(p %in% c(1,2,Inf))
assert_that(operation %in% c("train", "predict"))
if(operation == "predict") not_empty(memory)
class.type <- unique(labels)
#Calculate Order Matrix
if(operation == "train") {
class.prob <- Train(features, labels, class.type, k, p)
} else {
class.prob <- Predict(features, memory, labels, class.type, k, p)
}
#Predict classes
predicted.class <- data.frame(t(apply(class.prob, 1, function(row) c(type = class.type[which.max(row)], prob = max(row)))))
error <- accuracy <- NA
if(operation == "train"){
error <- table(predicted.class$type, labels)
accuracy <- mean(predicted.class$type == labels )
} else {
error <- table(predicted.class$type, features[,3])
accuracy <- mean(predicted.class$prob == features[,3])
df <- data.frame(class = predicted.class$type, prob = predicted.class$prob)
write.csv(df, file = "predictions.csv", row.names = TRUE)
}
return(list(pred.labels = predicted.class$type,
error = error,
accuracy = accuracy,
class.prob = predicted.class$prob))
}
param.vector <- list(class1 = list(n = 1000, a = 10, b = 20, theta = pi/6, step = 0.03, disturbance = 3,
cov.factor = 10, class.name = "A"),
class2 = list(n = 1000, a = 50, b = 20, theta = pi/6, step = 0.03, disturbance = 3,
cov.factor = 6, class.name = "B"),
class3 = list(n = 1000, a = 100, b = 20, theta = pi/6, step = 0.03, disturbance = 3,
cov.factor = 6, class.name = "C"))
features <- GenData(no.classes = 3, param.vector = param.vector)
features <- genWaves()
labels <- features$y
test <- genWaves(slice = 0.05, saveData = TRUE, savePlot = TRUE, seed = 12345) )
test <- genWaves(slice = 0.05, saveData = TRUE, savePlot = TRUE, seed = 12345)
test.summary <- KNNClassifier(features = test, memory = features, labels = labels,
operation = "test", k = best.k, p = 2)
test.summary <- KNNClassifier(features = test, memory = features, labels = labels,
operation = "predict", k = best.k, p = 2)
test.summary
test.summary$error
summary$error
k_seq
pdf("error_plot.pdf", width=6, height=5)
print(ggplot(data = error.DF) +
geom_point(aes(x = k, y = error)) +
geom_line(aes(x = k, y = error)) +
ggtitle("Error Rates with different K") +
scale_x_discrete(breaks= k_seq, labels=k_seq))
dev.off()
test.summary$error
gridder <- function(x, y, z, grid.x, grid.y, h.x, h.y) {
# error checking and filling in default values
n <- length(x)
stopifnot(length(y)==n, length(z)==n)
if(missing(grid.x)) {
grid.x <- seq(min(x),max(x),length=n)
}
if(missing(grid.y)) {
grid.y <- seq(min(y),max(y),length=n)
}
if(missing(h.x)) {
h.x <- 1.06*sd(x)/n^(1/5) # silverman's univariate rule of thumb
}
if(missing(h.y)) {
h.y <- 1.06*sd(y)/n^(1/5) # silverman's univariate rule of thumb
}
# the guts of the function come next; they make a 2-dimensional
# kernel-smoothed regression using independent gaussian kernels in
# each coordinate, from which we can estimate values of z above the
# regular grid:
grid.out <- expand.grid(grid.x,grid.y)  # make a regular grid
x.out <- grid.out[,1]                   # extract the x values
y.out <- grid.out[,2]                   # extract the y values
n.out <- length(x.out)
z.out <- c(NA,n.out)                    # set up a vector for the z values
for (i in 1:n.out) {                    # fill in the z's using a 2-dim
xdiffs <- x - x.out[i]                # gaussian kernel regression est
ydiffs <- y - y.out[i]
weights <- dnorm(xdiffs,sd=h.x)*dnorm(ydiffs,sd=h.y)
z.out[i] <- sum(z*weights)/sum(weights)
}
return(data.frame(x=x.out,y=y.out,z=z.out))
}
gridded.data <- gridder(possum$belly, possum$totlngth, possum$chest)
library(reshape2) # for melt
volcano
View(volcano)
volcano3d <- melt(volcano)
names(volcano3d) <- c("x", "y", "z")
View(volcano3d)
volcano <- volcano
?volcano
names(volcano3d) <- c("x", "y", "z")
volcano3d <- melt(volcano)
names(volcano3d) <- c("x", "y", "z")
v <- ggplot(volcano3d, aes(x, y, z = z))
v + stat_contour()
v
volcano3d <- melt(volcano)
names(volcano3d) <- c("x", "y", "z")
v <- ggplot(volcano3d, aes(x, y, z = z))
v + stat_contour()
v + stat_contour(bins = 2)
v + stat_contour(bins = 10)
v + stat_contour(binwidth = 2)
v + stat_contour(binwidth = 5)
v + stat_contour(binwidth = 10)
v + stat_contour(binwidth = 2, size = 0.5, colour = "grey50") +
stat_contour(binwidth = 10, size = 1)
v + stat_contour(aes(size = ..level..))
v + stat_contour(aes(colour = ..level..))
v + stat_contour(aes(colour = ..level..), size = 2) +
scale_colour_gradient(low = "brown", high = "white")
# Set aesthetics to fixed value
v + stat_contour(colour = "red")
v + stat_contour(size = 2, linetype = 4)
# Try different geoms
v + stat_contour(geom="polygon", aes(fill=..level..))
v + geom_tile(aes(fill = z)) + stat_contour()
read.table(file = "./predictions.csv", header = TRUE)
read.table(file = "./predictions.csv", header = TRUE, sep = ",")
library("assertthat")
library("mvtnorm")
source("./genData.R")
#--------------------------------------------------------------------------------
#calculate distance between a row and given dataset
CalcDist <- function(row, data, p = 2) {
#check is length of row is equal to # of columns of data
#assert_that(are_equal(length(as.numeric(row)), ncol(data)))
row.matrix <- matrix( rep(as.numeric(row), each = nrow(data)) , nrow = nrow(data))
return((rowSums( abs((data - row.matrix))^p )) ^ (1.0/p))
}
#--------------------------------------------------------------------------------
LabelCount <- function(vec, class.type){
freq <- vector()
for(class in class.type){
freq <- c(freq, sum(vec == class))
}
return(freq)
}
#--------------------------------------------------------------------------------
#Calculates order matrix (neighbors) for Training data
Train <- function(features, labels, class.type, k = 1, p = 2) {
#create dist matrix
dist.mat <- apply(features[,1:2], 1, function(row) CalcDist(row, features[,1:2], p))
#order.matrix
order.mat <- as.matrix(t(apply(dist.mat, 1, order))[,2:(k+1)])
class.mat <- matrix(0, nrow = nrow(order.mat), ncol = ncol(order.mat))
for(row in 1:nrow(order.mat)) {
for(col in 1:ncol(order.mat)) {
class.mat[row, col] <- labels[order.mat[row, col]]
}
}
class.prob <- as.data.frame(t(apply(X = class.mat, MARGIN =  1,
FUN = function(row){LabelCount(row, class.type)/k})))
names(class.prob) <- class.type
return(class.prob)
}
#--------------------------------------------------------------------------------
#Calculates order matrix (neighbors) for new data
Predict <- function(features, memory, labels, class.type, k = 1, p = 2) {
#calculate distance matrix
dist.mat <- apply( features[,1:2], 1, function(row) CalcDist(row, memory[,1:2], p))
#order.matrix
order.mat <- as.matrix(t(apply(dist.mat, 2, order))[,1:k])
class.mat <- matrix(0, nrow = nrow(order.mat), ncol = ncol(order.mat))
for(row in 1:nrow(order.mat)) {
for(col in 1:ncol(order.mat)) {
class.mat[row, col] <- labels[order.mat[row, col]]
}
}
class.prob <- as.data.frame(t(apply(X = class.mat, MARGIN =  1,
FUN = function(row){LabelCount(row, class.type)/k})))
names(class.prob) <- class.type
return(class.prob)
}
#--------------------------------------------------------------------------------
#Main function for classifying Data
KNNClassifier <- function(features, memory = NULL, labels,
operation = "train", k = 1, p = 2) {
#Input Validation
not_empty(features)
not_empty(labels)
is.count(k)
is.count(p)
assert_that(k %% 2 == 1)
assert_that(p %in% c(1,2,Inf))
assert_that(operation %in% c("train", "predict"))
if(operation == "predict") not_empty(memory)
class.type <- unique(labels)
#Calculate Order Matrix
if(operation == "train") {
class.prob <- Train(features, labels, class.type, k, p)
} else {
class.prob <- Predict(features, memory, labels, class.type, k, p)
}
#Predict classes
predicted.class <- data.frame(t(apply(class.prob, 1, function(row) c(type = class.type[which.max(row)], prob = max(row)))))
error <- accuracy <- NA
if(operation == "train"){
error <- table(predicted.class$type, labels)
accuracy <- mean(predicted.class$type == labels )
} else {
error <- table(predicted.class$type, features[,3])
accuracy <- mean(predicted.class$prob == features[,3])
df <- data.frame(class = predicted.class$type, prob = predicted.class$prob)
write.csv(df, file = "predictions.csv", row.names = TRUE)
}
return(list(pred.labels = predicted.class$type,
error = error,
accuracy = accuracy,
class.prob = predicted.class$prob))
}
#create dataset
features <- genWaves()
labels <- features$y
#trainig data with best k
k_seq <- seq.int(from = 1, to = 31, by = 2)
train_error <- sapply(k_seq,
function(k) {
(1 - KNNClassifier(features =  features, labels = labels, operation = "train", k = k)$accuracy)
})
#data frams of errors with differet k
error.DF <- data.frame(k = k_seq, error = train_error)
#plot
pdf("error_plot.pdf", width=6, height=5)
print(ggplot(data = error.DF) +
geom_point(aes(x = k, y = error)) +
geom_line(aes(x = k, y = error)) +
ggtitle("Error Rates with different K") +
scale_x_discrete(breaks= k_seq, labels=k_seq))
dev.off()
#choose best k
best.k <- error.DF$k[which.min(error.DF$error)]
#train with best k
summary <- KNNClassifier(features = features, memory = NULL, labels = labels,
operation = "train", k = best.k, p = 2)
best.k <- error.DF$k[which.min(error.DF$error)]
summary <- KNNClassifier(features = features, memory = NULL, labels = labels,
operation = "train", k = best.k, p = 2)
test.summary$error
test <- genWaves(slice = 0.05, saveData = TRUE, savePlot = TRUE, seed = 12345)
test.summary <- KNNClassifier(features = test, memory = features, labels = labels,
operation = "predict", k = best.k, p = 2)
test.summary$error
df <- data.frame(x1 = test$x1, x2 = test$x2, class = test.summary$pred.labels, prob = test.summary$class.prob)
df.melt <- ?melt
?melt
ggplot(data = df) +
geom_point(aes(x = x1, y =x2, color = class))
